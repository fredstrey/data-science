{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddcceda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping e arquivo CSV completados!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Primeiramente, vamos especificar a url do site e o headers\n",
    "#Vamos fazer o scrapping no site do mercado livre com \"Iphone\" na caixa de busca\n",
    "#O header é uma identificação do meu navegador, é usado para validação\n",
    "base_url = \"https://lista.mercadolivre.com.br/iphone#D[A:iphone]?page=\"\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"}\n",
    "#Vamos definir o numero de paginas de pesquisa nas quais faremos o scrapping\n",
    "num_pages = 5\n",
    "\n",
    "#Vamos criar o arquivo csv no qual salvaremos as informações obtidas do site\n",
    "csv_file = open(\"product_data.csv\", \"w\", newline=\"\")\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow([\"Product Name\", \"Price\"])\n",
    "\n",
    "#Faremos o loop de interação em todas as páginas especificadas em num_pages\n",
    "for page in range(1, num_pages + 1):\n",
    "    #Juntamos a url do site com o valor correspondente de cada pagina\n",
    "    url = base_url + str(page)\n",
    "    \n",
    "    #fazemos a solicitação para acessar a página\n",
    "    response = requests.get(url,headers)\n",
    "    \n",
    "    # criamos um objeto utilizando o beautifulsoup para analisar o html da página\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    #Buscamos o elemento da página que contem as informações que queremos buscar (neste caso é o título do anuncio e o preço à vista)\n",
    "    #para encontrar o elemento que buscamos, podemos utilizar a função \"inspecionar\" nativa dos navegadores de internet, e buscar pela informação que queremos no arquivo html do site\n",
    "    products = soup.find_all(\"div\", class_=\"ui-search-result__content-wrapper\")\n",
    "    \n",
    "    #Vamos localizar e extrair as informações que buscamos, que estão contidas no objeto armazenado em products \n",
    "    for product in products:\n",
    "        #Pegamos a informação do titulo do anuncio e salvamos na variavel \"name\"\n",
    "        name = product.find(\"h2\", class_=\"ui-search-item__title\").get_text(strip=True)\n",
    "        \n",
    "        #Pegamos a informação do preço do produto e salvamos na variavel \"price\"\n",
    "        price = product.find(\"span\", class_=\"andes-money-amount__fraction\").get_text(strip=True)\n",
    "        \n",
    "        #Salvamos as informações no arquivo csv\n",
    "        csv_writer.writerow([name, price])\n",
    "\n",
    "\n",
    "csv_file.close()\n",
    "\n",
    "print(\"Scraping e arquivo CSV completados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b41d135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
